(.venv) ➜  workload_network git:(main) ✗ k logs scheduler-plugins-scheduler-78b75ffb99-cr6r9                                            
I1011 02:36:22.111747       1 flags.go:64] FLAG: --allow-metric-labels="[]"
I1011 02:36:22.111959       1 flags.go:64] FLAG: --allow-metric-labels-manifest=""
I1011 02:36:22.112033       1 flags.go:64] FLAG: --authentication-kubeconfig=""
I1011 02:36:22.112041       1 flags.go:64] FLAG: --authentication-skip-lookup="false"
I1011 02:36:22.112054       1 flags.go:64] FLAG: --authentication-token-webhook-cache-ttl="10s"
I1011 02:36:22.112062       1 flags.go:64] FLAG: --authentication-tolerate-lookup-failure="true"
I1011 02:36:22.112066       1 flags.go:64] FLAG: --authorization-always-allow-paths="[/healthz,/readyz,/livez]"
I1011 02:36:22.112351       1 flags.go:64] FLAG: --authorization-kubeconfig=""
I1011 02:36:22.112461       1 flags.go:64] FLAG: --authorization-webhook-cache-authorized-ttl="10s"
I1011 02:36:22.112470       1 flags.go:64] FLAG: --authorization-webhook-cache-unauthorized-ttl="10s"
I1011 02:36:22.112475       1 flags.go:64] FLAG: --bind-address="0.0.0.0"
I1011 02:36:22.112483       1 flags.go:64] FLAG: --cert-dir=""
I1011 02:36:22.112493       1 flags.go:64] FLAG: --client-ca-file=""
I1011 02:36:22.112619       1 flags.go:64] FLAG: --config="/etc/kubernetes/scheduler-config.yaml"
I1011 02:36:22.112762       1 flags.go:64] FLAG: --contention-profiling="true"
I1011 02:36:22.112773       1 flags.go:64] FLAG: --disabled-metrics="[]"
I1011 02:36:22.112888       1 flags.go:64] FLAG: --feature-gates=""
I1011 02:36:22.113053       1 flags.go:64] FLAG: --help="false"
I1011 02:36:22.113062       1 flags.go:64] FLAG: --http2-max-streams-per-connection="0"
I1011 02:36:22.113068       1 flags.go:64] FLAG: --kube-api-burst="100"
I1011 02:36:22.113079       1 flags.go:64] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
I1011 02:36:22.113084       1 flags.go:64] FLAG: --kube-api-qps="50"
I1011 02:36:22.113102       1 flags.go:64] FLAG: --kubeconfig=""
I1011 02:36:22.113107       1 flags.go:64] FLAG: --leader-elect="true"
I1011 02:36:22.113112       1 flags.go:64] FLAG: --leader-elect-lease-duration="15s"
I1011 02:36:22.113121       1 flags.go:64] FLAG: --leader-elect-renew-deadline="10s"
I1011 02:36:22.113125       1 flags.go:64] FLAG: --leader-elect-resource-lock="leases"
I1011 02:36:22.113130       1 flags.go:64] FLAG: --leader-elect-resource-name="kube-scheduler"
I1011 02:36:22.113134       1 flags.go:64] FLAG: --leader-elect-resource-namespace="kube-system"
I1011 02:36:22.113142       1 flags.go:64] FLAG: --leader-elect-retry-period="2s"
I1011 02:36:22.113147       1 flags.go:64] FLAG: --log-flush-frequency="5s"
I1011 02:36:22.113152       1 flags.go:64] FLAG: --log-text-info-buffer-size="0"
I1011 02:36:22.113160       1 flags.go:64] FLAG: --log-text-split-stream="false"
I1011 02:36:22.113164       1 flags.go:64] FLAG: --logging-format="text"
I1011 02:36:22.113169       1 flags.go:64] FLAG: --master=""
I1011 02:36:22.113174       1 flags.go:64] FLAG: --permit-address-sharing="false"
I1011 02:36:22.113178       1 flags.go:64] FLAG: --permit-port-sharing="false"
I1011 02:36:22.113186       1 flags.go:64] FLAG: --pod-max-in-unschedulable-pods-duration="5m0s"
I1011 02:36:22.113191       1 flags.go:64] FLAG: --profiling="true"
I1011 02:36:22.113195       1 flags.go:64] FLAG: --requestheader-allowed-names="[]"
I1011 02:36:22.113208       1 flags.go:64] FLAG: --requestheader-client-ca-file=""
I1011 02:36:22.113213       1 flags.go:64] FLAG: --requestheader-extra-headers-prefix="[x-remote-extra-]"
I1011 02:36:22.113223       1 flags.go:64] FLAG: --requestheader-group-headers="[x-remote-group]"
I1011 02:36:22.113232       1 flags.go:64] FLAG: --requestheader-username-headers="[x-remote-user]"
I1011 02:36:22.113241       1 flags.go:64] FLAG: --secure-port="10259"
I1011 02:36:22.113246       1 flags.go:64] FLAG: --show-hidden-metrics-for-version=""
I1011 02:36:22.113253       1 flags.go:64] FLAG: --tls-cert-file=""
I1011 02:36:22.113258       1 flags.go:64] FLAG: --tls-cipher-suites="[]"
I1011 02:36:22.113266       1 flags.go:64] FLAG: --tls-min-version=""
I1011 02:36:22.113271       1 flags.go:64] FLAG: --tls-private-key-file=""
I1011 02:36:22.113275       1 flags.go:64] FLAG: --tls-sni-cert-key="[]"
I1011 02:36:22.113282       1 flags.go:64] FLAG: --v="1"
I1011 02:36:22.113288       1 flags.go:64] FLAG: --version="false"
I1011 02:36:22.113295       1 flags.go:64] FLAG: --vmodule=""
I1011 02:36:22.113303       1 flags.go:64] FLAG: --write-config-to=""
I1011 02:36:22.604127       1 serving.go:380] Generated self-signed cert in-memory
W1011 02:36:22.604897       1 client_config.go:659] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I1011 02:36:23.826139       1 topologyscheduler.go:230] "TopologyScheduling: Initializing plugin"
I1011 02:36:23.827140       1 server.go:154] "Starting Kubernetes Scheduler" version="v0.0.20241010"
I1011 02:36:23.827166       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1011 02:36:23.957835       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I1011 02:36:23.957920       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I1011 02:36:23.957957       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1011 02:36:23.957971       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1011 02:36:23.957987       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1011 02:36:23.957995       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1011 02:36:23.958112       1 secure_serving.go:213] Serving securely on [::]:10259
I1011 02:36:23.958175       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1011 02:36:24.158395       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I1011 02:36:24.158497       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1011 02:36:24.258929       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1011 02:36:25.289067       1 topologyscheduler.go:45] "TopologyScheduling: QueueSort called" pod1="mp-test-1-kw47g" pod2="mp-test-2-hj2rd"
I1011 02:36:25.289154       1 topologyscheduler.go:63] "TopologyScheduling: QueueSort: Pods have resource label" pod1="mp-test-1-kw47g" Resource1="mid-layer" pod2="mp-test-2-hj2rd" Resource2="top-layer"
I1011 02:36:25.417904       1 topologyscheduler.go:85] "TopologyScheduling: QueueSort: Got order level" order1=2 order2=3
I1011 02:36:25.418023       1 topologyscheduler.go:45] "TopologyScheduling: QueueSort called" pod1="mp-test-0-qqhxh" pod2="mp-test-1-kw47g"
I1011 02:36:25.418038       1 topologyscheduler.go:63] "TopologyScheduling: QueueSort: Pods have resource label" pod1="mp-test-0-qqhxh" Resource1="bottom-layer" pod2="mp-test-1-kw47g" Resource2="mid-layer"
I1011 02:36:25.546859       1 topologyscheduler.go:85] "TopologyScheduling: QueueSort: Got order level" order1=1 order2=2
I1011 02:36:25.558972       1 topologyscheduler.go:45] "TopologyScheduling: QueueSort called" pod1="mp-test-2-hj2rd" pod2="mp-test-1-kw47g"
I1011 02:36:25.558999       1 topologyscheduler.go:63] "TopologyScheduling: QueueSort: Pods have resource label" pod1="mp-test-2-hj2rd" Resource1="top-layer" pod2="mp-test-1-kw47g" Resource2="mid-layer"
I1011 02:36:25.687130       1 topologyscheduler.go:85] "TopologyScheduling: QueueSort: Got order level" order1=3 order2=2
I1011 02:36:25.687281       1 topologyscheduler.go:122] "TopologyScheduling: PreFilter: get Node used" node="worker-4" allocatable=1
I1011 02:36:25.687357       1 topologyscheduler.go:122] "TopologyScheduling: PreFilter: get Node used" node="worker-5" allocatable=1
I1011 02:36:25.687368       1 topologyscheduler.go:122] "TopologyScheduling: PreFilter: get Node used" node="worker-1" allocatable=1
I1011 02:36:25.687375       1 topologyscheduler.go:122] "TopologyScheduling: PreFilter: get Node used" node="worker-2" allocatable=1
I1011 02:36:25.687384       1 topologyscheduler.go:122] "TopologyScheduling: PreFilter: get Node used" node="worker-0" allocatable=1
I1011 02:36:25.687391       1 topologyscheduler.go:122] "TopologyScheduling: PreFilter: get Node used" node="worker-3" allocatable=1
I1011 02:36:25.816443       1 topologyscheduler.go:134] "TopologyScheduling: PreFilter: got network metrics" data={"Bandwidth":{"worker-0":{"worker-1":951.46,"worker-2":27.59,"worker-3":9.2,"worker-4":8.96,"worker-5":31.38},"worker-1":{"worker-0":884.77,"worker-2":49.7,"worker-3":60.18,"worker-4":2.41,"worker-5":43.83},"worker-2":{"worker-0":17.1,"worker-1":4.06,"worker-3":1017.52,"worker-4":1.59,"worker-5":1.77},"worker-3":{"worker-0":2.06,"worker-1":17.82,"worker-2":1019.19,"worker-4":1.17,"worker-5":3.69},"worker-4":{"worker-0":47.18,"worker-1":33.55,"worker-2":24.12,"worker-3":27.89,"worker-5":912.85},"worker-5":{"worker-0":89.13,"worker-1":2.45,"worker-2":27.89,"worker-3":1.64,"worker-4":913.29}},"Latency":{"worker-0":{"worker-1":0.53,"worker-2":120.14,"worker-3":120.35,"worker-4":115.77,"worker-5":117.49},"worker-1":{"worker-0":1.34,"worker-2":120.78,"worker-3":122.93,"worker-4":117.5,"worker-5":116.85},"worker-2":{"worker-0":124.75,"worker-1":120.99,"worker-3":0.61,"worker-4":233.35,"worker-5":228.25},"worker-3":{"worker-0":121.22,"worker-1":121.06,"worker-2":5.64,"worker-4":229.84,"worker-5":229.55},"worker-4":{"worker-0":115.88,"worker-1":117.16,"worker-2":228.99,"worker-3":229.27,"worker-5":0.8},"worker-5":{"worker-0":118.24,"worker-1":116.74,"worker-2":228.93,"worker-3":229.12,"worker-4":0.69}}}
I1011 02:36:25.945096       1 topologyscheduler.go:141] "TopologyScheduling: PreFilter: got CRD Info" spec=[{"resource":"top-layer","dependencies":[{"resource":"mid-layer","bandwidth":{"requests":50,"limits":1},"latency":{"requests":50,"limits":999}}]},{"resource":"mid-layer","dependencies":[{"resource":"bottom-layer","bandwidth":{"requests":50,"limits":1},"latency":{"requests":10,"limits":999}}]},{"resource":"bottom-layer"}] status=[{"Resource":"bottom-layer","Order":1,"Phase":"Pending","SchedulingInfo":[{"PodName":"mp-test-0-qqhxh","NodeName":""}]},{"Resource":"mid-layer","Order":2,"Phase":"Pending","SchedulingInfo":[{"PodName":"mp-test-1-kw47g","NodeName":""}]},{"Resource":"top-layer","Order":3,"Phase":"Pending","SchedulingInfo":[{"PodName":"mp-test-2-hj2rd","NodeName":""}]}]
I1011 02:36:25.945282       1 topologyscheduler.go:158] "TopologyScheduling: PreFilter: get dependencies" resourceList=["bottom-layer","mid-layer","top-layer"] dependencies=[[1,50,999,10],[1,50,999,50]]
I1011 02:36:25.945312       1 topologyscheduler.go:161] "TopologyScheduling: PreFilter: got best node set" nodeSet=[] bestScore="<internal error: json: unsupported value: -Inf>"
I1011 02:36:25.945412       1 topologyscheduler.go:177] "TopologyScheduler: Filter called" pod="mp-test-0-qqhxh" node="worker-4"
I1011 02:36:25.945428       1 topologyscheduler.go:189] "TopologyScheduler: Filter: read data from PreFilter" data=[]
I1011 02:36:25.945712       1 topologyscheduler.go:177] "TopologyScheduler: Filter called" pod="mp-test-0-qqhxh" node="worker-5"
I1011 02:36:25.945732       1 topologyscheduler.go:189] "TopologyScheduler: Filter: read data from PreFilter" data=[]
I1011 02:36:25.945855       1 topologyscheduler.go:177] "TopologyScheduler: Filter called" pod="mp-test-0-qqhxh" node="worker-1"
I1011 02:36:25.945869       1 topologyscheduler.go:189] "TopologyScheduler: Filter: read data from PreFilter" data=[]
I1011 02:36:25.946114       1 topologyscheduler.go:177] "TopologyScheduler: Filter called" pod="mp-test-0-qqhxh" node="worker-2"
I1011 02:36:25.946125       1 topologyscheduler.go:189] "TopologyScheduler: Filter: read data from PreFilter" data=[]
I1011 02:36:25.946181       1 topologyscheduler.go:177] "TopologyScheduler: Filter called" pod="mp-test-0-qqhxh" node="worker-0"
I1011 02:36:25.946193       1 topologyscheduler.go:189] "TopologyScheduler: Filter: read data from PreFilter" data=[]
I1011 02:36:25.946436       1 topologyscheduler.go:177] "TopologyScheduler: Filter called" pod="mp-test-0-qqhxh" node="worker-3"
I1011 02:36:25.946448       1 topologyscheduler.go:189] "TopologyScheduler: Filter: read data from PreFilter" data=[]
E1011 02:36:26.076334       1 runtime.go:79] Observed a panic: runtime.boundsError{x:0, y:0, signed:true, code:0x0} (runtime error: index out of range [0] with length 0)
goroutine 430 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x2602f40, 0xc000945f20})
        /workspace/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:75 +0x85
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc000d8dc00?})
        /workspace/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:49 +0x6b
panic({0x2602f40?, 0xc000945f20?})
        /usr/local/go/src/runtime/panic.go:770 +0x132
sigs.k8s.io/scheduler-plugins/pkg/topologyscheduling.(*TopologyScheduling).Filter(0xc000633b00?, {0x408946?, 0x135?}, 0xc000da7380, 0xc000e23688, 0xc000dbbd40)
        /workspace/pkg/topologyscheduling/topologyscheduler.go:199 +0x99b
k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*instrumentedFilterPlugin).Filter(0xc0004859a0, {0x2ba6900, 0xc000cff770}, 0xc000da7380, 0xc000e23688, 0xc000dbbd40)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/runtime/instrumented_plugins.go:37 +0x6b
k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*frameworkImpl).runFilterPlugin(0x26d0e40?, {0x2ba6900?, 0xc000cff770?}, {0x2b88cf8?, 0xc0004859a0?}, 0x1d143c5?, 0xc000394dc0?, 0xc000d902f0?)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/runtime/framework.go:879 +0x294
k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*frameworkImpl).RunFilterPlugins(0xc0005886c8, {0x2ba6900, 0xc000cff770}, 0xc000da7380, 0xc000e23688, 0xc000dbbd40)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/runtime/framework.go:863 +0x27d
k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*frameworkImpl).RunFilterPluginsWithNominatedPods(0xc0005886c8, {0x2ba6938, 0xc000958230}, 0xc000da7380, 0xc000e23688, 0xc000dbbd40)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/runtime/framework.go:992 +0x1c8
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).findNodesThatPassFilters.func1(0x40ad3e?)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:609 +0xe4
k8s.io/kubernetes/pkg/scheduler/framework/parallelize.Parallelizer.Until.func1(0x6)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/parallelize/parallelism.go:60 +0x46
k8s.io/client-go/util/workqueue.ParallelizeUntil.func1()
        /workspace/vendor/k8s.io/client-go/util/workqueue/parallelizer.go:90 +0xfa
created by k8s.io/client-go/util/workqueue.ParallelizeUntil in goroutine 426
        /workspace/vendor/k8s.io/client-go/util/workqueue/parallelizer.go:76 +0x1fb
panic: runtime error: index out of range [0] with length 0 [recovered]
        panic: runtime error: index out of range [0] with length 0

goroutine 430 [running]:
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc000d8dc00?})
        /workspace/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:56 +0xcd
panic({0x2602f40?, 0xc000945f20?})
        /usr/local/go/src/runtime/panic.go:770 +0x132
sigs.k8s.io/scheduler-plugins/pkg/topologyscheduling.(*TopologyScheduling).Filter(0xc000633b00?, {0x408946?, 0x135?}, 0xc000da7380, 0xc000e23688, 0xc000dbbd40)
        /workspace/pkg/topologyscheduling/topologyscheduler.go:199 +0x99b
k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*instrumentedFilterPlugin).Filter(0xc0004859a0, {0x2ba6900, 0xc000cff770}, 0xc000da7380, 0xc000e23688, 0xc000dbbd40)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/runtime/instrumented_plugins.go:37 +0x6b
k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*frameworkImpl).runFilterPlugin(0x26d0e40?, {0x2ba6900?, 0xc000cff770?}, {0x2b88cf8?, 0xc0004859a0?}, 0x1d143c5?, 0xc000394dc0?, 0xc000d902f0?)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/runtime/framework.go:879 +0x294
k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*frameworkImpl).RunFilterPlugins(0xc0005886c8, {0x2ba6900, 0xc000cff770}, 0xc000da7380, 0xc000e23688, 0xc000dbbd40)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/runtime/framework.go:863 +0x27d
k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*frameworkImpl).RunFilterPluginsWithNominatedPods(0xc0005886c8, {0x2ba6938, 0xc000958230}, 0xc000da7380, 0xc000e23688, 0xc000dbbd40)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/runtime/framework.go:992 +0x1c8
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).findNodesThatPassFilters.func1(0x40ad3e?)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:609 +0xe4
k8s.io/kubernetes/pkg/scheduler/framework/parallelize.Parallelizer.Until.func1(0x6)
        /workspace/vendor/k8s.io/kubernetes/pkg/scheduler/framework/parallelize/parallelism.go:60 +0x46
k8s.io/client-go/util/workqueue.ParallelizeUntil.func1()
        /workspace/vendor/k8s.io/client-go/util/workqueue/parallelizer.go:90 +0xfa
created by k8s.io/client-go/util/workqueue.ParallelizeUntil in goroutine 426
        /workspace/vendor/k8s.io/client-go/util/workqueue/parallelizer.go:76 +0x1fb